# Установка библиотеки gymnasium
!pip install gymnasium

# Импорт необходимых библиотек
import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from collections import deque
import random

# Определение устройств для вычислений (GPU или CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Определение класса Actor
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, max_action):
        super(Actor, self).__init__()
        self.layer1 = nn.Linear(state_dim, 256)
        self.layer2 = nn.Linear(256, 256)
        self.layer3 = nn.Linear(256, action_dim)
        self.max_action = max_action

    def forward(self, state):
        x = torch.relu(self.layer1(state))
        x = torch.relu(self.layer2(x))
        x = self.max_action * torch.tanh(self.layer3(x))
        return x

# Определение класса Critic
class Critic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(Critic, self).__init__()
        self.layer1 = nn.Linear(state_dim + action_dim, 256)
        self.layer2 = nn.Linear(256, 256)
        self.layer3 = nn.Linear(256, 1)

    def forward(self, state, action):
        x = torch.cat([state, action], 1)
        x = torch.relu(self.layer1(x))
        x = torch.relu(self.layer2(x))
        x = self.layer3(x)
        return x

# Определение класса DDPG
class DDPG:
    def __init__(self, state_dim, action_dim, max_action):
        self.actor = Actor(state_dim, action_dim, max_action).to(device)
        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)

        self.critic = Critic(state_dim, action_dim).to(device)
        self.critic_target = Critic(state_dim, action_dim).to(device)
        self.critic_target.load_state_dict(self.critic.state_dict())
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)

        self.max_action = max_action
        self.replay_buffer = deque(maxlen=1000000)
        self.batch_size = 64

    def select_action(self, state):
        state = torch.FloatTensor(state.reshape(1, -1)).to(device)
        return self.actor(state).cpu().data.numpy().flatten()

    def train(self, replay_buffer, iterations, discount=0.99, tau=0.005):
        critic_losses = []
        for _ in range(iterations):
            # Выборка из реплей буфера
            experiences = replay_buffer.sample(self.batch_size)
            state, action, next_state, reward, not_done = zip(*experiences)

            state = torch.FloatTensor(np.array(state)).to(device)
            action = torch.FloatTensor(np.array(action)).to(device)
            next_state = torch.FloatTensor(np.array(next_state)).to(device)
            reward = torch.FloatTensor(np.array(reward)).to(device).unsqueeze(1)
            not_done = torch.FloatTensor(np.array(not_done)).to(device).unsqueeze(1)

            # Вычисление целевого Q-значения
            target_Q = self.critic_target(next_state, self.actor_target(next_state))
            target_Q = reward + (not_done * discount * target_Q).detach()

            # Текущее Q-значение
            current_Q = self.critic(state, action)

            # Вычисление потерь критика
            critic_loss = nn.MSELoss()(current_Q, target_Q)
            critic_losses.append(critic_loss.item())

            # Оптимизация критика
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            self.critic_optimizer.step()

            # Вычисление потерь актера
            actor_loss = -self.critic(state, self.actor(state)).mean()

            # Оптимизация актера
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            self.actor_optimizer.step()

            # Обновление целевых моделей
            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):
                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)

            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):
                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)

        return critic_losses

# Определение метода sample для deque
class ReplayBuffer:
    def __init__(self, max_size=1000000):
        self.buffer = deque(maxlen=max_size)

    def add(self, experience):
        self.buffer.append(experience)

    def sample(self, batch_size):
        idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)
        return [self.buffer[i] for i in idx]

# Определение функции обучения
def train_ddpg(env, agent, episodes=1000, max_timesteps=200, render=False):
    replay_buffer = ReplayBuffer()
    rewards = []
    losses = []
    for episode in range(episodes):
        state, _ = env.reset()
        episode_reward = 0
        for t in range(max_timesteps):
            if render:
                env.render()

            action = agent.select_action(state)
            next_state, reward, done, _, _ = env.step(action)

            replay_buffer.add((state, action, next_state, reward, float(not done)))

            state = next_state
            episode_reward += reward

            if done:
                break

        if len(replay_buffer.buffer) > agent.batch_size:
            critic_losses = agent.train(replay_buffer, iterations=10)
            losses.extend(critic_losses)

        rewards.append(episode_reward)
        print(f"Эпизод: {episode}, Награда: {episode_reward}")

    return rewards, losses

# Визуализация результатов
def plot_results(rewards, losses):
    plt.figure(figsize=(10, 5))
    plt.plot(rewards)
    plt.title("Награды за эпизод")
    plt.xlabel("Эпизод")
    plt.ylabel("Награда")
    plt.show()

    plt.figure(figsize=(10, 5))
    plt.plot(losses)
    plt.title("Потери критика за эпизод")
    plt.xlabel("Эпизод")
    plt.ylabel("Потери")
    plt.show()

# Функция тестирования
def test_agent(env, agent, episodes=10, max_timesteps=200, render=True):
    rewards = []
    for episode in range(episodes):
        state, _ = env.reset()
        episode_reward = 0
        for t in range(max_timesteps):
            if render:
                env.render()

            action = agent.select_action(state)
            next_state, reward, done, _, _ = env.step(action)

            state = next_state
            episode_reward += reward

            if done:
                break

        rewards.append(episode_reward)
        print(f"Тестовый эпизод: {episode}, Награда: {episode_reward}")

    return rewards

# Основной код
if __name__ == "__main__":
    env = gym.make("Pendulum-v1")
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    max_action = float(env.action_space.high[0])

    agent = DDPG(state_dim, action_dim, max_action)
    rewards, losses = train_ddpg(env, agent, episodes=1000)

    plot_results(rewards, losses)

    # Тестирование обученного агента
    test_rewards = test_agent(env, agent, episodes=10)

    # Отчет
    print("\nОтчет:")
    print("Описание реализованного алгоритма:")
    print("Реализовали алгоритм DDPG для задачи управления маятником. Алгоритм состоит из двух нейронных сетей: Actor и Critic.")
    print("Actor отвечает за выбор действий, а Critic оценивает Q-значение для пары состояние-действие.")
    print("Алгоритм обучается на основе опыта, который собирается в реплей буфере.")

    print("\nГрафики обучения:")
    print("Графики наград и потерь критика построены.")

    print("\nВыводы о работе агента:")
    print(f"Средняя награда за обучение: {np.mean(rewards[-100:])}")
    print(f"Средняя награда за тестирование: {np.mean(test_rewards)}")
    print("Агент должен удерживать маятник в вертикальном положении, что должно привести к средней награде -200 или выше.")
    print("Стабильность обучения можно оценить по графику наград. Если график показывает плавное увеличение наград, то обучение стабильно.")
    print("Время сходимости зависит от количества эпизодов и параметров алгоритма.")
